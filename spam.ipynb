{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Required imports and some constants setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "MARKER = 'Date: '\n",
    "BASE = 'datasets'\n",
    "FILTERED = '_filtered'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial data cleaning - potentially not required, maybe even harmful to the models performance. Still, I decided to classify emails only based on their contents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 999 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "non_spam_folders = ('easy_ham1', 'easy_ham2', 'hard_ham1', 'hard_ham2')\n",
    "spam_folders = list(map(lambda n: f'spam{n}', range(1, 5)))\n",
    "\n",
    "\n",
    "def leave_contents_only(filepath):\n",
    "    path = Path(filepath)\n",
    "    filtered_path = f'{BASE}/{FILTERED}'\n",
    "    category_path = f'{filtered_path}/{path.parent.name}'\n",
    "\n",
    "    if not os.path.exists(filtered_path):\n",
    "        os.mkdir(filtered_path)\n",
    "    if not os.path.exists(category_path):\n",
    "        os.mkdir(category_path)\n",
    "\n",
    "    with open(filepath, 'r', errors='ignore') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "        try:\n",
    "            index = contents.index(MARKER)\n",
    "            contents = contents[index + len(MARKER):]\n",
    "        except ValueError:\n",
    "            print(f'Marker {MARKER} not found in {filepath}.')\n",
    "\n",
    "    new_path = f'{category_path}/{path.name}'\n",
    "\n",
    "    with open(new_path, 'w') as file:\n",
    "        file.write(contents)\n",
    "\n",
    "\n",
    "def create_clean_files(folders):\n",
    "    for folder in folders:\n",
    "        if os.path.exists(f'{BASE}/{FILTERED}/{folder}'):\n",
    "            return\n",
    "\n",
    "        path = f'{BASE}/{folder}/*'\n",
    "        filepaths = glob.glob(path)\n",
    "\n",
    "        for filepath in filepaths:\n",
    "            leave_contents_only(filepath)\n",
    "\n",
    "create_clean_files(non_spam_folders)\n",
    "create_clean_files(spam_folders)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get splitted training and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def get_file_names(folders):\n",
    "    files = []\n",
    "\n",
    "    for folder in folders:\n",
    "        for (dirpath, dirnames, filenames) in os.walk(f'{BASE}/{FILTERED}/{folder}'):\n",
    "            files += [os.path.join(dirpath, file) for file in filenames]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "spam_files = get_file_names(spam_folders)\n",
    "non_spam_files = get_file_names(non_spam_folders)\n",
    "y = np.hstack(\n",
    "    (np.ones(len(spam_files), dtype=bool),\n",
    "     np.zeros(len(non_spam_files), dtype=bool)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_files + non_spam_files, y, test_size=.2, random_state=RANDOM_STATE, stratify=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vect = vectorizer.transform(X_test).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First choice - SVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "svc_clf = SVC()\n",
    "\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "svc_clf.fit(X_train_vect, y_train)\n",
    "\n",
    "preds = svc_clf.predict(X_test_vect)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1023   87]\n",
      " [ 562  197]]\n",
      "[0.64542587 0.69366197]\n",
      "[0.92162162 0.25955204]\n",
      "[0.75918367 0.37775647]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "conf = confusion_matrix(y_test, preds)\n",
    "\n",
    "display(conf)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds)\n",
    "\n",
    "display(precision)\n",
    "display(recall)\n",
    "display(f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second choice - RFC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1109,    1],\n       [ 645,  114]], dtype=int64)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([0.6322691 , 0.99130435])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([0.9990991 , 0.15019763])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([0.77444134, 0.26086957])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_train_vect, y_train)\n",
    "\n",
    "preds = rfc.predict(X_test_vect)\n",
    "\n",
    "conf = confusion_matrix(y_test, preds)\n",
    "\n",
    "display(conf)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds)\n",
    "\n",
    "display(precision)\n",
    "display(recall)\n",
    "display(f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
